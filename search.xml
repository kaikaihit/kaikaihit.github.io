<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Method</title>
    <url>/2020/08/14/Method/</url>
    <content><![CDATA[<p><strong>初始化并安装所需组件：</strong></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">hexo init      # 初始化</span><br><span class="line">npm install    # 安装组件</span><br></pre></td></tr></table></figure>

<p>完成后依次输入下面命令，<strong>启动本地服务器进行预览</strong>：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">hexo g   # 生成页面</span><br><span class="line">hexo s   # 启动预览</span><br></pre></td></tr></table></figure>

<p>首先<strong>安装 hexo-deployer-git</strong>：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>

<p>然后<strong>修改 _config.yml</strong> 文件末尾的 Deployment 部分，修改成如下：</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:用户名/用户名.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>

<p>清楚缓存等文件并重新发布网站：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hexo clean   # 清除缓存文件等</span><br><span class="line">hexo g       # 生成页面</span><br><span class="line">hexo s       # 启动预览</span><br></pre></td></tr></table></figure>

<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">hexo deploy #部署发布</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>【知识图谱系列合集】一、知识提取</title>
    <url>/2020/08/14/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%B3%BB%E5%88%97%E5%90%88%E9%9B%86%E3%80%91%E4%B8%80%E3%80%81%E7%9F%A5%E8%AF%86%E6%8F%90%E5%8F%96/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200810201749355.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="知识图谱构建流程"><br>数据主要有三类：结构化数据、半结构化数据和非结构化数据。<br>知识提取的目的：通过自动化或者半自动化的技术抽取出可用的知识单元，知识单元包括实体、属性和关系，并以此为基础，形成一系列高质量的事实表达，为上层模式层的构建奠定基础。<br>**<em>本文主要讲述知识图谱构建过程中的知识提取，实体抽取、属性抽取和关系抽取。**</em></p>
<p><strong><em>1.实体抽取</em></strong><br>实体抽取也称为命名实体学习，指的是从原始数据语料中自动识别出命名实体。实体是知识图谱中最基本的元素，所以实体抽取是知识抽取中最重要且最基础的一步。实体抽取的方法可以分为以下四种。</p>
<p><strong>1.1基于百科站点或垂直站点提取</strong><br>很常规基本的提取方法。<br>从百科类站点的标题和链接中提取实体名。优点是可以得到开放互联网中最常见的实体名，其缺点是对于中低频的覆盖率低。与一般性通用的网站相比，垂直类站点的实体提取可以获取特定领域的实体。例如从豆瓣各频道(音乐、读书、电影等)获取各种实体列表。这种方法主要是基于爬取技术来实现和获取。</p>
<p><strong>1.2基于规则与词典的方法</strong><br>需要为目标实体编写模板，然后在原始语料中进行匹配。<br>早期的实体抽取是在限定文本领域、限定语义单元类型的条件下进行的，主要采用的是基于规则与词典的方法，例如使用已定义的规则，抽取出文本中的人名、地名、组织机构名、特定时间等实体。然而，基于规则模板的方法不仅需要依靠大量的专家来编写规则或模板，覆盖的领域范围有限，而且很难适应数据变化的新需求。</p>
<p><strong>1.3基于统计机器学习的方法</strong><br>通过机器学习的方法对原始语料进行训练，然后再利用训练好的模型去识别实体。<br>鉴于基于规则与词典实体的局限性，为具更有可扩展性，相关研究人员将机器学习中的监督学习算法用于命名实体的抽取问题上。近年来随着深度学习的兴起应用，基于深度学习的命名实体识别得到广泛应用。</p>
<p><strong>1.4面向开放域的抽取方法</strong><br>是面向海量的Web语料。<br>针对如何从少量实体实例中自动发现具有区分力的模式，进而扩展到海量文本去给实体做分类与聚类的问题。</p>
<p><strong><em>2.属性和属性值抽取</em></strong><br>属性提取的任务是为每个本体语义类构造属性列表，而属性值提取则为一个语义类的实体附加属性值。属性和属性值的抽取能够形成完整的实体概念的知识图谱维度。常见的属性和属性值抽取方法包括从百科类站点中提取，从垂直网站中进行包装器归纳，从网页表格中提取，以及利用手工定义或自动生成的模式从句子和查询日志中提取。</p>
<p>常见的语义类/ 实体的常见属性/ 属性值可以通过解析百科类站点中的半结构化信息（如维基百科的信息盒和百度百科的属性表格）而获得。尽管通过这种简单手段能够得到高质量的属性，但同时需要采用其它方法来增加覆盖率（即为语义类增加更多属性以及为更多的实体添加属性值）。</p>
<p><strong><em>3.关系抽取</em></strong><br>关系抽取的目标是解决实体语义链接的问题。关系的基本信息包括参数类型、满足此关系的元组模式等。分为开放式实体关系抽取，基于联合推理的实体关系抽取等。</p>
<p><strong>3.1开放式实体关系抽取</strong><br>开放式实体关系抽取可分为二元开放式关系抽取和n元开放式关系抽取。</p>
<p>在二元开放式关系抽取中，早期的研究有KnowItAll与TextRunner系统，在准确率与召回率上表现一般。有文献提出了一种基于Wikipedia的OIE方法WOE，经自监督学习得到抽取器，准确率较TextRunner有明显的提高。针对WOE的缺点，有文献提出了第二代OIE ReVerb系统，以动词关系抽取为主。有文献提出了第三代OIE系统OLLIE(open language learning for information extraction)，尝试弥补并扩展OIE的模型及相应的系统，抽取结果的准确度得到了增强。</p>
<p>然而，基于语义角色标注的OIE分析显示：英文语句中40%的实体关系是n元的[32]，如处理不当，可能会影响整体抽取的完整性。有文献提出了一种可抽取任意英文语句中n元实体关系的方法KPAKEN，弥补了ReVerb的不足。但是由于算法对语句深层语法特征的提取导致其效率显著下降，并不适用于大规模开放域语料的情况。</p>
<p><strong>3.2基于联合推理的实体关系抽取</strong><br>联合推理的关系抽取中的典型方法是马尔可夫逻辑网，它是一种将马尔可夫网络与一阶逻辑相结合的统计关系学习框架，同时也是在OIE中融入推理的一种重要实体关系抽取模型。</p>
<p>联合推理的关系抽取中的典型方法是马尔可夫逻辑网MLN，它是一种将马尔可夫网络与一阶逻辑相结合的统计关系学习框架，同时也是在OIE中融入推理的一种重要实体关系抽取模型。基于该模型，有文献提出了一种无监督学习模型StatSnowball，不同于传统的OIE，该方法可自动产生或选择模板生成抽取器。在StatSnowball的基础上，有文献提出了一种实体识别与关系抽取相结合的模型EntSum，主要由扩展的CRF命名实体识别模块与基于StatSnowball的关系抽取模块组成，在保证准确率的同时也提高了召回率。有文献提出了一种简易的Markov逻辑TML(tractable Markov logic)，TML将领域知识分解为若干部分，各部分主要来源于事物类的层次化结构，并依据此结构，将各大部分进一步分解为若干个子部分，以此类推。TML具有较强的表示能力，能够较为简洁地表示概念以及关系的本体结构。</p>
<p><strong><em>4.语义类提取</em></strong><br>语义类抽取是指从文本中自动抽取信息来构造语义类并建立实体和语义类的关联, 作为实体层面上的规整和抽象。以下介绍有效的语义类抽取方法，包含三个模块：并列度相似计算、上下位关系提取以及语义类生成 。</p>
<p><strong>4.1并列度相似计算</strong><br>当前主流的并列相似度计算方法有分布相似度法和模式匹配法。</p>
<p>分布相似度方法：经常出现在类似的上下文环境中的两个词具有语义上的相似性。分布相似度方法的实现分三个步骤：第一步，定义上下文；第二步，把每个词表示成一个特征向量，向量每一维代表一个不同的上下文，向量的值表示本词相对于上下文的权重；第三步，计算两个特征向量之间的相似度，将其作为它们所代表的词之间的相似度。 、</p>
<p>模式匹配法：基本思路是把一些模式作用于源数据，得到一些词和词之间共同出现的信息，然后把这些信息聚集起来生成单词之间的相似度。模式可以是手工定义的，也可以是根据一些种子数据而自动生成的。</p>
<p>分布相似度法和模式匹配法都可以用来在数以百亿计的句子中或者数以十亿计的网页中抽取词的相似性信息。。</p>
<p><strong>4.2上下位关系提取</strong><br>该模块从文档中抽取词的上下位关系信息，生成（下义词，上义词）数据对，例如（狗，动物）、（悉尼，城市）。这种方法的主要缺点包括：并不是所有的分类词条都代表上位词，例如百度百科中“狗”的开放分类“养殖”就不是其上位词；生成的关系图中没有权重信息，因此不能区分同一个实体所对应的不同上位词的重要性；覆盖率偏低，即很多上下位关系并没有包含在百科站点的分类信息中。</p>
<p><strong>4.3语义类生成</strong><br>该模块包括聚类和语义类标定两个子模块。聚类的结果决定了要生成哪些语义类以及每个语义类包含哪些实体，而语义类标定的任务是给一个语义类附加一个或者多个上位词作为其成员的公共上位词。此模块依赖于并列相似性和上下位关系信息来进行聚类和标定。有些研究工作只根据上下位关系图来生成语义类，但经验表明并列相似性信息对于提高最终生成的语义类的精度和覆盖率都至关重要。</p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
      </tags>
  </entry>
  <entry>
    <title>【知识图谱系列合集】三、实体对齐.md</title>
    <url>/2020/08/14/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%B3%BB%E5%88%97%E5%90%88%E9%9B%86%E3%80%91%E4%B8%89%E3%80%81%E5%AE%9E%E4%BD%93%E5%AF%B9%E9%BD%90-md/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200810204859835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong><em>导读：</em></strong></p>
<ul>
<li>1.<a href="https://blog.csdn.net/zkzbhh/article/details/107921135">知识提取</a>：从原始数据语料中自动识别出命名实体。实体是知识图谱中最基本的元素。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">https:<span class="comment">//blog.csdn.net/zkzbhh/article/details/107921135</span></span><br></pre></td></tr></table></figure>

<ul>
<li>2.<a href="https://blog.csdn.net/zkzbhh/article/details/107921258">知识表示</a>：介绍了知识表示学习的常见的代表模型：距离模型、单层神经网络模型、能量模型、双线性模型、张量神经网络模型、矩阵分解模型。翻译模型（Trans模型）,TransH模型、TransR模型、TransD模型、TranSpare模型、TransA模型、TransG模型、KG2E模型。</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">https:<span class="comment">//blog.csdn.net/zkzbhh/article/details/107921258</span></span><br></pre></td></tr></table></figure>
<p><strong>本篇文章，我们介绍实体对齐的相关内容，包括实体消歧和共指消歧。</strong></p>
<p><strong><em>一、实体消歧</em></strong><br><strong>含义</strong>：实体消歧的本质在于一个词有很多可能的意思，也就是在不同的上下文中所表达的含义不太一样。</p>
<p><strong>例子</strong>：<br><img src="https://img-blog.csdnimg.cn/20200810205414690.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>“我的手机是苹果”和”我喜欢吃苹果”这两个句子中的“苹果”代表的含义是不一样的。前者代表是手机、后者代表是水果。</p>
<p><strong>具体实现</strong>：</p>
<p>“美国一家高科技公司，经典的产品有Iphone手机”转换为向量 [公式] ；“水果的一种，一般产自于…”转换为向量 [公式] ；然后将“今天苹果发布了新的手机”中“苹果”的上下文“今天，发布了新的手机”转换为向量 [公式] ;我们只要将 [公式] 分别与 [公式] 和 [公式] 计算相似度，然后对比 [公式] 和 [公式] ;相似度高的，我们则将其看作“苹果”的真实语义。</p>
<p><strong><em>二、共指消歧</em></strong></p>
<p>共指消解，也叫指代消解。很难因为自然语言充满歧义，因此必须使用多种信号和知识来消除歧义。需要基于对周围世界的了解才能明白这些指代，而这种知识很难编码到计算机中。举个例子：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">乔布斯改变了我们听音乐、购买音乐、打电话、使用手机、看电影等各方面的体验。当他去世时，奥巴马称他为最伟大的创新者。</span><br></pre></td></tr></table></figure>
<p>上面这句话中的他指代的是乔布斯。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">乔布斯改变了我们听音乐、购买音乐、打电话、使用手机、看电影等各方面的体验。当他听到这个消息，奥巴马致电慰问他的妻子。</span><br></pre></td></tr></table></figure>
<p>那这段话中的他指代的是奥巴马。</p>
<p>指代消解主要有两个步骤。第一步是指代识别（mention detection），即找出句子中所有的指代，这一步相对简单。第二步才是进行真正的指代消解（coreference resolution），这一步比较难。</p>
<p><strong>1.指代识别</strong></p>
<p>指代识别阶段尽量保召回率，保留所有找到的可能是指代的词，都参与后期的指代消解。如果一个指代没有找到它的共同指代（coreference），则说明这个指代是孤立的（singleton mention），有可能是指代识别阶段找到的不是指代的词，直接舍弃。</p>
<p><strong>2.指代消解</strong></p>
<p>指代消解发展至今，经历了四种不同的方法，分别是Rule-based、Mention pair、Mention Ranking。</p>
<p><strong>Rule-based方法</strong>：1976年，Hobbs提出了基于规则的朴素算法，被后人称为Hobbs算法。该方法有9个步骤，包含了很多规则，非常繁琐。Hobbs算法虽然是基于规则的，但在当时取得了不错的效果，现在也常常作为该领域的baseline模型。但是因为该方法是基于规则的，有很多指代消解没法解决。</p>
<p><strong>Mention pair方法</strong>：该方法把指代消解问题转化为一个二分类问题。从左到右遍历句子，每找到一个指代，就把它和前面找到的每个指代作为一个pair，问分类器这个pair是否指代同一个实体，如果是的话，就把它们连起来。二分类的损失就是交叉熵。很简单的一个模型。</p>
<p><strong>Mention Ranking</strong>：每个指代同时和前面所有指代打分，用softmax归一化，找出概率最大的先行词，添加一条连边。注意需要添加一个NA节点，因为有的指代可能第一次出现，前面没有先行词，或者这个指代根本就不是一个真正的指代。</p>
<p>前面的内容都是假设我们计算好了任意两个指代是coreference的概率，那么，如何来计算这个概率呢？主要有三种方法，分别是Non-neural statistical classifier、Simple neural network和More advanced model using LSTMs, attention。</p>
<p><strong>A. Non-neural statistical classifie</strong>r。统计机器学习方法，抽取每个指代的各种特征，然后用机器学习分类器来计算两个指代是coreference的概率。这里面的特征包括人称、性别一致性，语义相容性等等。</p>
<p><strong>B. Neural Coref Model</strong>。输入是候选先行词和当前指代词的词向量，还需要加入一些额外的特征（Additional Feature），也就是上面统计机器学习方法里用到的一些特征。中间是FFNN，即全连接网络，最后输出两个指代是coreference的概率。</p>
<p><strong>C. End-to-end Model</strong>。end2end模型是目前指代消解的SOTA模型，它把指代识别和指代消解两个任务融合到一起，用一个模型来解决。</p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱 实体对齐</tag>
      </tags>
  </entry>
  <entry>
    <title>【知识图谱系列合集】二、知识表示.md</title>
    <url>/2020/08/14/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%B3%BB%E5%88%97%E5%90%88%E9%9B%86%E3%80%91%E4%BA%8C%E3%80%81%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA-md/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200810202240689.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="知识图谱构建流程"><br>将结构化、半结构化和非结构化数据中的实体、关系和属性进行提取。之后就要进行知识表示。本文主要介绍知识表示的概念、常见的代表模型：距离模型、单层神经网络模型、能量模型、双线性模型、张量神经网络模型、矩阵分解模型和Trans系列模型。</p>
<p><strong><em>说明：</em></strong></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">摘自刘知远：知识表示学习研究进展（DOI;<span class="number">10.7544</span>/issn1000-<span class="number">1239.2016</span>.<span class="number">20160020</span>)</span><br></pre></td></tr></table></figure>
<p><strong><em>导读：</em></strong></p>
<ul>
<li><a href="https://blog.csdn.net/zkzbhh/article/details/107921135">知识提取</a></li>
</ul>
<p><strong><em>目录：</em></strong></p>
<ul>
<li>一、知识表示学习概念</li>
<li>二、知识表示学习主要方法</li>
</ul>
<p><strong><em>一.知识表示学习的概念</em></strong><br><strong>one-hot representation</strong>：将研究对象表示为向量，该向量只有某一维非零，其它维度上的值均为0，独热表示是信息检索和搜索引擎中广泛使用的词袋模型的基础。优点是无需学习过程，简单高效，在信息检索和自然语言处理中得到广泛应用。缺点是会丢失大量有用信息，无法有效表示短文本、容易受到数据稀疏问题的影响。</p>
<p><strong>表示学习的目标</strong>：通过机器学习将研究对象的语义信息表示为稠密低维实值向量。将实体e和关系r表示为两个不同向量，在向量空间中，通过欧式距离或余弦距离等方式，计算任意两个对象之间的语义相似度。</p>
<p>知识表示学习得到的分布式表示，我们可以快速计算实体间的语义相似度，这对于自然语言处理和信息检索的很多任务有重要意义；我们还可以知识图谱补全，构建大规模知识图谱，需要不断补充实体间的关系，利用知识表示学习模型，我们可以预测2个实体的关系；除此之外，还可以用于关系抽取，自动问答，实体链指等任务，展现出巨大的应用潜力。</p>
<p><strong>知识表示学习的优点</strong>：显著提升计算效率，知识库的三元组表示实际就是基于独热表示的，在这种表示方式下，需要设计专门的图算法计算实体间的语义和推理关系，计算复杂度高、可扩展性差。而表示学习得到的分布式表示，能够高效地实现语义相似度计算等操作，显著提升计算效率；有效缓解数据稀疏；实现异质信息融合。</p>
<p><strong><em>二.知识表示学习的主要方法</em></strong><br>知识表示学习的代表模型，包括距离模型、单层神经网络模型、能量模型、双线性模型、张量神经网络模型、矩阵分解模型和翻译模型，还有最重要的Trans系列模型。</p>
<p><strong>1.距离模型（SE）</strong></p>
<p>每个实体用d维的向量表示，所有实体被投影到同一个d维向量空间中，同时，距离模型还未每个关系r定义2个矩阵，用于三元组中头实体和尾实体的投影操作。最后，距离模型为每个三元组（h,r,t）定义了如下损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810202439393.png" alt="距离模型的损失函数"><br>距离模型的<strong>损失函数</strong><br><img src="https://img-blog.csdnimg.cn/2020081020250574.png" alt="在这里插入图片描述"><br>距离模型将头实体向量和尾实体向量通过关系r的2个矩阵投影到r的对应空间中，然后在该空间中计算两投影向量的距离。这个距离反映了2个实体在关系r下的语义相似度，他们距离越小，说明这2个实体存在这种关系。</p>
<p>距离模型能够利用学习得到的知识表示进行链接预测，即通过计算，找到让两实体距离最近的关系矩阵。</p>
<p>距离模型的缺陷：协同性差，无法精确刻画两个实体之间的语义联系</p>
<p><strong>2.单层神经网络模型(SLM）</strong></p>
<p>采用单层神经网络的非线性操作，来减轻距离模型无法协同精确刻画实体与关系的语义联系的问题。SLM为每个三元组（h,r,t）定义了评分函数：<br><img src="https://img-blog.csdnimg.cn/20200810202546740.png" alt="在这里插入图片描述"><br>SLM是SE模型的改进版本，但是它的非线性操作仅提供了实体和关系之间比较微弱的联系，与此同时，引进了更高的计算复杂度。</p>
<p><strong>3.能量模型（SME）</strong></p>
<p>语义匹配能量模型，提出更复杂的操作，寻找实体和关系之间的语义联系。在SME中，每个实体和关系都用低维向量表示，在此基础上，SME定义若干投影矩阵，刻画实体与关系的内在联系，SME为每个三元组（h,r,t）定义了2种评分函数，分别是线性形式：<br><img src="https://img-blog.csdnimg.cn/20200810202602385.png" alt="在这里插入图片描述"><br>和双线性形式：<br><img src="https://img-blog.csdnimg.cn/20200810202614946.png" alt="在这里插入图片描述"><br>此外，也可以用三阶张量代替SME的双线性形式。</p>
<p><strong>4.双线性模型（LFM）</strong></p>
<p>隐变量模型提出利用基于关系的双线性变换，刻画实体和关系之间的二阶关系，LFM为每个三元组（h,r,t）定义了如下双线性评分函数：<br><img src="https://img-blog.csdnimg.cn/20200810202720407.png" alt="在这里插入图片描述"></p>
<p><strong>5.张量神经网络模型（NTN）</strong></p>
<p>张量神经网络模型的基本思想是，用双线性张量取代传统神经网络中的线性变换层，在不同的维度下将头、尾实体向量联系起来。基本思想如下图：<br><img src="https://img-blog.csdnimg.cn/20200810202751191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>张量神经网络模型<br>NTN为每个三元组（h,r,t）定义了如下评分函数，评价2个实体之间存在的某个特定关系r的可能性：<br><img src="https://img-blog.csdnimg.cn/20200810202805711.png" alt="在这里插入图片描述"><br>NTN中的实体向量是该实体中所有单词向量的平均值，这样做的好处是，实体中的单词数量远小于实体数量，可以充分重复利用单词向量构建实体表示，降低实体表示学习的稀疏性问题，增强不同实体的语义联系。</p>
<p>NTN的缺陷：虽然能够更精确地刻画实体和关系的复杂语义联系，但复杂度非常高，需要大量三元组样例才能得到充分学习，NTN在大规模稀疏知识图谱上的效果较差。</p>
<p><strong>6.矩阵分解模型（RESACL）</strong></p>
<p>知识库三元组构成了一张大的张量X，如果三元组（h,r,t）存在，则X=1，否则为0.张量分解旨在将每个三元组（h,r,t）对应的张量值X分解为实体和关系表示，使得X尽量地接近于LML</p>
<p>RESACL的基本思想与前述LFM类似，不同之处在于，RESACL会优化张量中的所有位置，包括0的位置；而LFM只会优化知识库中存在的三元组。</p>
<p><strong>7.Trans系列模型</strong><br><strong>7.1TransH模型</strong><br>在处理1-N，N-1，N-N复杂关系时的局限性，TransH模型提出让一个实体在不同的关系下拥有不同的表示。<br><img src="https://img-blog.csdnimg.cn/20200810203540774.png" alt="在这里插入图片描述"><br>损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810203556520.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200810203617402.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="TransH model"><br><strong>7.2TransR/CTransR模型</strong></p>
<p>虽然TransH模型使每个实体在不同关系下拥有不同的表示，它仍然假设实体和关系处于相同的语义空间R中，在一定程度上限制了TransH的表达能力。TransR模型认为，一个实体是多种属性的综合体，不同关系关注实体的不同属性。TransR认为不同的关系拥有不同的语义空间，对每个三元组，首先应将实体投影到对应的关系空间中，然后再建立从头实体到尾实体的翻译关系。</p>
<p><img src="https://img-blog.csdnimg.cn/20200810204641888.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p>上图是TransR模型的简单示例，对于每个三元组（h,r,t），我们首先将实体向量向关系r空间投影，原来在实体空间中与头（圆圈表示）、尾实体相似的实体（三角形表示），在关系r空间内被区分开了。具体而言，对于每一个关系r，TransR表示如下：<br><img src="https://img-blog.csdnimg.cn/20200810203754256.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200810203800147.png" alt="[公式]"><br>因此 TransR定义了如下损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810203823358.png" alt="在这里插入图片描述"><br>相关研究表明，某些关系还可以进行更细致的划分。于是，Lin等人进一步提出了CTransR模型，通过把关系r对应的实体对的向量差值 进行聚类，将关系r细分为多个子关系.CTransR模型为每一个子关系分别学习向量表示，对于每个三元组，定义了如下损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810203853566.png" alt="在这里插入图片描述"><br><strong>TransR模型的缺点：</strong><br>（1）在同一个关系r下，头、尾实体共享相同的投影矩阵。然而，一个关系的头、尾实体的类型或属性可能差异巨大。例如（美国，总统，奥巴马），美国和奥巴马的类型不同，一个是国家，一个是人物。<br>（2）从实体空间到关系空间的投影是实体和关系的交互过程。因此TransR让投影矩阵仅与关系有关是不合理的。<br>（3）与TransE和TransH相比，TransR引入了空间投影，使得TransR模型参数急剧增加，计算复杂率大大提高。</p>
<p><strong>7.3TransD模型</strong></p>
<p>为了解决TransR模型的缺点，Ji等人提出了TransD模型，如下图所示，给定三元组,TransD模型设置了2个分别将头实体和尾实体投影到关系空间的投影矩阵，具体定义如下：<br><img src="https://img-blog.csdnimg.cn/20200810203940111.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200810203954891.png" alt="[公式] ， [公式]"><br>利用2个投影向量构建投影矩阵，解决了原来TransR模型参数过多的问题。最后，TransD模型定义了损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810204006764.png" alt="[公式]"><br><strong>7.4TransSpare模型</strong></p>
<p>知识库中实体和关系的异质性和不平衡性是制约知识表示学习的难题：<br>（1）异质性：知识库中某些关系可能会与大量的实体有连接，而某些关系则可能仅仅与少量实体有连接。</p>
<p>（2）不均衡性：在某些关系中，头实体和尾实体的种类和数量可能差别巨大。</p>
<p>为了解决实体和关系的异质性。TransSparse提出使用稀疏矩阵 [公式] 的稀疏度由关系r连接的实体对数量决定。这里头、尾实体共享同一个投影矩阵，投影矩阵的稀疏度定义如下：<br><img src="https://img-blog.csdnimg.cn/20200810204037748.png" alt="在这里插入图片描述"><br>这样，投影向量可定义为：<br><img src="https://img-blog.csdnimg.cn/20200810204049578.png" alt="[公式] , [公式]"><br>为了解决关系的不平衡性问题，TranSparse对于头实体和尾实体分别使用2个不同的投影矩阵.;两者的稀疏度定义如下：<br><img src="https://img-blog.csdnimg.cn/20200810204103696.png" alt="在这里插入图片描述"><br>这样，投影向量可定义为：<br><img src="https://img-blog.csdnimg.cn/20200810204120186.png" alt="[公式] , [公式]"><br>TranSparse对于以上2种形式，均定义如下损失函数：<br><img src="https://img-blog.csdnimg.cn/20200810204130645.png" alt="在这里插入图片描述"><br><strong>7.5TransA模型</strong><br><strong>加粗样式</strong><br>Xiao等人提出TransA模型，将损失函数中的距离度量改为马氏距离，并为每一维学习不同的权重，对于每个三元组.TransA模型定义了如下评分函数：<br><img src="https://img-blog.csdnimg.cn/20200810204428630.png" alt="在这里插入图片描述"><br>如下图所示：<br><img src="https://img-blog.csdnimg.cn/20200810204419823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><strong>7.6 TransG模型</strong></p>
<p>TransG模型提出使用高斯混合模型描述头、尾实体之间的关系。该模型认为，一个关系会对应多种语义，每种语义用一个高斯分布来刻画，其中I表示单位矩阵。<br><img src="https://img-blog.csdnimg.cn/20200810204517797.png" alt="在这里插入图片描述"><br>TransG模型与传统模型的对比如下图所示。其中三角形表示正确的尾实体，圆形表示错误的尾实体。<br><img src="https://img-blog.csdnimg.cn/20200810204459833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>图(a)中为传统模型示例，由于将关系r的所有语义混为一谈，导致错误的实体无法被区分开，而图(b)所示，TransG模型通过考虑关系r的不同语义，形成多个高斯分布，就能区分出正确和错误实体。</p>
<p><strong>7.7KG2E模型</strong></p>
<p>知识库中的关系和实体的语义 本身具有不确定性，这在过去模型中被忽略了。因此He等人提出KG2E，使得高斯分布来表示实体和关系。其中高斯分布的均值表示的是实体或关系在语义空间中的中心位置，而高斯分布的协方差则表示该实体或关系的不确定度。下图为KG2E的模型示例，每个圆圈代表不同实体与关系的表示，其中圈圈大小表示的是不同实体或关系的不确定度。</p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱</tag>
        <tag>知识表示</tag>
      </tags>
  </entry>
  <entry>
    <title>【知识图谱系列合集】四、知识存储.md</title>
    <url>/2020/08/14/%E3%80%90%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1%E7%B3%BB%E5%88%97%E5%90%88%E9%9B%86%E3%80%91%E5%9B%9B%E3%80%81%E7%9F%A5%E8%AF%86%E5%AD%98%E5%82%A8-md/</url>
    <content><![CDATA[<p><img src="https://img-blog.csdnimg.cn/20200810210037352.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>
<p><strong><em>导读：</em></strong></p>
<ul>
<li>1.<a href="https://blog.csdn.net/zkzbhh/article/details/107921135">知识提取</a>：从原始数据语料中自动识别出命名实体。实体是知识图谱中最基本的元素。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">https:<span class="comment">//blog.csdn.net/zkzbhh/article/details/107921135</span></span><br></pre></td></tr></table></figure></li>
<li>2.<a href="https://blog.csdn.net/zkzbhh/article/details/107921258">知识表示</a>：介绍了知识表示学习的常见的代表模型：距离模型、单层神经网络模型、能量模型、双线性模型、张量神经网络模型、矩阵分解模型。翻译模型（Trans模型）,TransH模型、TransR模型、TransD模型、TranSpare模型、TransA模型、TransG模型、KG2E模型。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">https:<span class="comment">//blog.csdn.net/zkzbhh/article/details/107921258</span></span><br></pre></td></tr></table></figure></li>
<li>3.<a href="https://blog.csdn.net/zkzbhh/article/details/107921782">实体对齐</a>：介绍实体消歧和共指消歧的方法。<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">https:<span class="comment">//blog.csdn.net/zkzbhh/article/details/107921782</span></span><br></pre></td></tr></table></figure>
<strong>本篇文章，主要介绍知识图谱的知识存储。</strong></li>
</ul>
<p><strong><em>1.RDF、RDFS和OWL数据模型</em></strong></p>
<p>RDF、RDFS和OWL是W3C[1]推荐的本体描述用护眼，也是知识图谱中数据的常用存储格式，它们都是基于XML[2]编写的元数据[3]，是用于计算机传输数据，让机器可理解，而不是面向用户展示的数据模型。</p>
<p><strong>(1)RDF</strong></p>
<p>RDF本质是一个数据模型，它提供了一个统一的标准描述web资源，资源指的是类、属性、实例等。RDF在形式上表示为三元组。用于描述具体事物及关系，RDF也可以表示为一张带有标记的有向图，图中有节点和边，节点对应实体，边对应关系或者属性，关系指的是实体之间，实体与属性之间的关系。</p>
<p><strong>RDF中的三元组Triple：</strong><br><img src="https://img-blog.csdnimg.cn/20200810210302969.png" alt="在这里插入图片描述"></p>
<p><strong>带有资源标识符URI的三元组：</strong><br><img src="https://img-blog.csdnimg.cn/20200810210200426.png" alt="在这里插入图片描述"><br>声明命名空间ex = <a href="http://ex.org/%E5%92%8Cex-schema">http://ex.org/和ex-schema</a> = <a href="http://ex.org/schema%EF%BC%8C%E4%BA%8E%E6%98%AF%E5%B8%A6%E6%9C%89%E8%B5%84%E6%BA%90%E6%A0%87%E8%AF%86%E7%AC%A6%E7%9A%84URI%E4%B8%89%E5%85%83%E7%BB%84%E5%B0%B1%E5%8F%AF%E4%BB%A5%E8%A1%A8%E7%A4%BA%E4%B8%BA%E4%B8%8B%E5%9B%BE%E5%BD%A2%E5%BC%8F%E3%80%82">http://ex.org/schema，于是带有资源标识符的URI三元组就可以表示为下图形式。</a></p>
<p><strong>声明前缀的三元组：</strong><br><img src="https://img-blog.csdnimg.cn/20200810210239944.png" alt="在这里插入图片描述"></p>
<p>RDF以三元组的形式描述资源，简洁明了，但是有着语义表达能力的缺陷。RDF中没有定义类、属性等词汇。RDF只能是对具体的事物进行描述，缺乏抽象能力，无法对同一个类别的事物进行定义和描述。RDF可以描述实体、实体的属性以及他们之间的关系，但是无法描述类与类之间的关系，类的属性等</p>
<p><strong>(2)RDFS</strong></p>
<p>RDFS在RDF的基础上定义了类（class）、属性（property）以及关系（relation）来描述资源，并且通过属性的定义域（domain）和值域（range）来约束资源。RDFS在数据层（data）的基础上引入了模式层（schema），模式层定义了一种约束规则，而数据层是在这种规则下的一个实例填充。</p>
<p>RDFS相比于RDF语义表达能力有所提升，但RDFS依旧有语义表达的缺陷。在RDFS中关于类与类之间的关系它只能声明子类关系，无法声明互斥类的关系，也无法声明多个类、实例、属性是否等价。</p>
<p><strong>(3)OWL</strong></p>
<p>OWL是对RDFS关于描述资源词汇的一个扩展，OWL中添加了额外的预定于词汇来描述资源，具备更好的语义表达能力。在OWL中可以声明资源的等价性，属性的传递性、互斥性、函数性、对称性等等，具体见OWL的词汇扩展。</p>
<p>本体和知识图谱在构建过程中，数据的存储常以RDF格式存放。而基于RDF数据的结构化查询语言SPARQL，可以实现对三元组的查询。在SPARQL中，常以“？”来表示变量或者资源标识符，select子句检索指定资源的资源标识符，where子句限定资源的由来。</p>
<p><strong><em>2.基于RDF的存储</em></strong></p>
<p>大部分开放的知识图谱，都是以RDF形式对外开放。</p>
<p><strong>RDF结构：</strong></p>
<p>RDF 为描述资源提供的基本元素有 IRI，字面值和空节点 (blank node)。IRI 就是一个符合特定语法的 UINICODE 字符串，跟 URL 的形式比较类似。其实 URL 属于 IRI 的一种。字面值可以理解为像时间、人名、数字等常量的表示，由字符串和表示数据类型的 IRI 构成。例如数字 1 的字面值可以表示为”1”^^xs:integer，其中 xs:integer 是表示整型数据类型的 IRI。空节点是指没有 IRI 的匿名节点。一般是 RDF 内部使用的一个特殊结构，不可被引用。</p>
<p><strong>一个 RDF 数据集由一组相关的三元组的组成。</strong>由于这个三元组集合可以抽象为一张 graph，因此也称为 RDF graph。</p>
<p><strong>例如，</strong>用 RDF 描述一本书，RDF 字典就需要定义一本书需要包含作者、书名、页数、出版时间、语言类型等。RDF 字典定义了数据建模的元数据项，这些元数据项主要包括两种类型 class 和 property。Class 是指对象实例的集合，可以理解为面向对象编程里的 class；Property 还分为两种子类型：一个是表示 class 的属性 (attribute)，另一个是表示多个 class 之间的关系 (relationship)。</p>
<p>另外，RDF 字典的定义自身也是一个 RDF graph。这也是说明 RDF 是自描述的数据模型，是一种 schema-free 的数据模型。</p>
<p><strong><em>3.基于图数据库的存储</em></strong></p>
<p>图数据库是一种非关系型数据库，以解决现有关系数据库的局限性。图模型明确地列出了数据节点之间的依赖关系，而关系模型和其他 NoSQL 数据库模型则通过隐式连接来链接数据。图数据库从设计上，就是可以简单快速地检索难以在关系系统中建模的复杂层次结构的。</p>
<p><strong>以neo4j为例介绍。</strong></p>
<p>数据存储形式：主要是 <strong>节点（node）和 边（edge）</strong> 来组织数据。node可以代表知识图谱中的实体，edge可以用来代表实体间的关系，关系可以有方向，两端对应开始节点和结束节点。另外，可以在node上加一个或多个<strong>标签（Node Label）表示实体的分类</strong>，以及一个键值对集合来表示该实体除了关系属性之外的一些额外属性。关系也可以附带额外的属性。</p>
<p>查询语言Cypher：neo4j采用自己设计的查询语言cypher，其特点和sql有很多相似的地方。<strong>match、where、return</strong>是最常用到的关键词：</p>
<ul>
<li>match: 相当于 sql中的select，用来说明<strong>查询匹配的数据模式</strong>（或者说图模式）</li>
<li>where: 用来限制node或者关系中部分属性的属性值，从而返回我们想要的数据</li>
<li>return: 返回节点或者关系</li>
</ul>
<p><strong>例子：</strong><br><img src="https://img-blog.csdnimg.cn/20200810210539178.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3premJoaA==,size_16,color_FFFFFF,t_70" alt="图片来自网络，侵权删"></p>
]]></content>
      <categories>
        <category>知识图谱</category>
      </categories>
      <tags>
        <tag>知识图谱 知识存储</tag>
      </tags>
  </entry>
  <entry>
    <title>hahah</title>
    <url>/2020/08/16/hahah/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>开玩笑</category>
      </categories>
      <tags>
        <tag>无</tag>
      </tags>
  </entry>
</search>
